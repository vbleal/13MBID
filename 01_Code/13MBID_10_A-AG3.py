# -*- coding: utf-8 -*-
"""Copia de 13MBID_10_A-AG3-BetancourtVictor_v07.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jwdmuhxapbqZMIHJ_6Scmmz6hUd9QgXZ

![](https://keystoneacademic-res.cloudinary.com/image/upload/element/14/146414_VIU_Cover_Cover.jpg)

---
# **13MBID_10_A**

# **Metodolog√≠as de Gesti√≥n y Dise√±o de Proyectos Big Data**

# **M√°ster en Big Data y Data Science**

[Universidad Internacional de Valencia (VIU)](https://www.universidadviu.com/int/maestria-universitaria-en-big-data-y-visual-analytics)

Curso 2022-2023. Edici√≥n Oct-2022

Alumno: **Victor David Betancourt Leal**

Repositorio: [GitHub ](https://github.com/vbleal/13MBID)

victor.betancourt@alumnos.viu.es

Profr.: **Dr. Horacio Kuna**

---
# **Actividad 3. Preparaci√≥n. Modelado. Evaluaci√≥n. Despliegue**

# **Sprint 2**
---

>üí° *Any Scrum without working product at the end of a Sprint, is a failed Scrum.*
‚ÄïJeff Sutherland, Scrum Co-Founder

# üóÇÔ∏è **√çndice**

* [1. Introducci√≥n](#1)

    
* [2. Settings](#2)


* [3. Informaci√≥n del Dataset](#3)


* [4. Verificaci√≥n de la Calidad](#4)


* [5. Preparaci√≥n de los Datos](#5)


* [6. Conclusiones](#6)


* [Bibliograf√≠a](#7)

---
<a id="1"></a>
# üéØ **Introducci√≥n**
---

##  Objetivo

El objetivo del presente Notebook consiste en desarrollas las Fases 4, 5, y 6 de la [Metodolog√≠a CRISP-DM](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-introduction-crisp-dm), las cuales se corresponden con el ***Sprint 2*** del proyecto. Recu√©rdese que las 6 fases de CRISP-DM son:

1.	[Comprensi√≥n del Negocio](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-business-understanding)

2.	[Comprensi√≥n de los Datos](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-data-understanding)

3.	[Preparaci√≥n de los Datos](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-data-preparation)

4.	[Modelado](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-modeling)

5.	[Evaluaci√≥n](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-evaluation)

6.	[Implementaci√≥n](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-deployment)


Se retomar√°n algunos puntos de la fase ***Preparaci√≥n de los Datos***, correspondientes al ***Sprint 1***.

Adicionalmente, se han incorporado elementos caracter√≠sticos de la Metodolog√≠a √Ågil ***SCRUM***, mediante el uso de la tecnolog√≠a [Azure Boards](https://azure.microsoft.com/en-us/).

Finalmente, se ha puesto a disposici√≥n el material utilizado a lo largo de este documento, en un repositorio de [GitHub](https://github.com/vbleal/13MBID), con la siguiente estructura, remembrando un poco el dise√±o de [Cookiecutter](https://drivendata.github.io/cookiecutter-data-science/)

![](https://raw.githubusercontent.com/vbleal/13MBID/main/Imag/Estructura_Ficheros.jpg)

---
<a id="2"></a>
# ‚öôÔ∏è **Settings**
---

## Importar Librer√≠as
"""

# Librer√≠as para Archivos
from google.colab import files
from google.colab import drive

# Commented out IPython magic to ensure Python compatibility.
# Librer√≠as
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

# %matplotlib inline

import seaborn as sns

# Instalar Dtale
!pip install Dtale

# Dtale
import dtale
import dtale.app as dtale_app

# Para usar Dtale en Colab
dtale_app.USE_COLAB = True

# Commented out IPython magic to ensure Python compatibility.
# Reinstalar Matplotlib por si Dtale lo desinstala
import matplotlib.pyplot as plt

# %matplotlib inline

# Instalar paquete Sweet Viz
!pip install sweetviz

# Importar Librer√≠a
import sweetviz

"""## Funciones Importar-Exportar"""

# Importar
# Funci√≥n para Cargar los Ficheros
def cargar_csv(nombre_archivo):
    # Abre el cuadro de di√°logo para seleccionar el archivo CSV
    print(f"Selecciona el archivo {nombre_archivo} desde tu explorador de archivos:")
    uploaded = files.upload()

    # Selecciona el archivo y gu√°rdalo en un DataFrame
    if nombre_archivo in uploaded.keys():
        df = pd.read_csv(nombre_archivo, sep=';')
        print(f"Fichero '{nombre_archivo}' subido correctamente y guardado en un DataFrame")
        return df
    else:
        print(f"Error: No se encontr√≥ el archivo {nombre_archivo}")
        return None

# Exportar
#from google.colab import files

# Funci√≥n para Exportar los DataFrames en CSV a Local
def descargar_dataframe_csv(dataframe, nombre_archivo):
    # Convertir el DataFrame a CSV
    csv_data = dataframe.to_csv(sep=";", index=False)
  
    # Crear un archivo temporal para guardar el contenido del CSV
    with open(nombre_archivo, 'w') as f:
        f.write(csv_data)
  
    # Descargar y guardar el archivo en la PC del usuario
    files.download(nombre_archivo)

"""## Importar Datasets

**Datos de Inscripciones (modificado)**
"""

# Datos Inscripciones
# Cargar Fichero
df_inscripciones = cargar_csv("datos_inscripciones_mod.csv")

# Imprimir Primeras Filas
if df_inscripciones is not None:
    print("\nDatos de Inscripciones (modificado):")
    print(df_inscripciones.shape)
    
df_inscripciones.head(1)



"""**Datos de Cursado (modificado)**"""

# Datos Cursado
# Cargar Fichero
df_cursado = cargar_csv("datos_cursado_mod.csv")

# Imprimir Primeras Filas
if df_cursado is not None:
    print("\nDatos de Cursado (modificado):")
    print(df_cursado.shape)
    
df_cursado.head(1)



"""**Datos Acad√©micos (modificado)**"""

# Datos Acad√©micos
# Cargar Fichero
df_academicos = cargar_csv("datos_academicos_mod.csv")

# Imprimir Primeras Filas
if df_academicos is not None:
    print("\nDatos de Cursado (modificado):")
    print(df_academicos.shape)
    
df_academicos.head(1)



"""---
<a id="5"></a>
# üõ†Ô∏è **Sprint 1: Preparaci√≥n de los Datos**
---

La fase de ***Preparaci√≥n de los Datos*** prosigue a la Comprensi√≥n de los Datos, y consta de **5 etapas**:

1. Selecci√≥n de Atributos

2. Limpieza de los Datos

3. Construcci√≥n de Datos

4. Integraci√≥n de los Datos

5. Formateo de los Datos

El alcance del presente Notebook corresponder√° √∫nicamente hasta la ***Selecci√≥n de los Atributos***, con ayuda de los datasets generados en la Fase de Comprensi√≥n.

## Selecci√≥n de Atributos

Los atributos de las reglas generadas en la fase de comprensi√≥n de los datos son: 

* err_formato_matricula (**todos**)
* regla_fechas_ingreso (**inscripciones**)
* regla_estado_inscripcion (**cursado**)
* regla_verificacion_calidad (**academicos**)

Se proceder√° a eliminar estas columnas en cada dataset.

**Datos de Inscripciones**
"""

# Eliminar Columnas
col_eliminar_inscripciones = ['plan_estudios', 'version_plan', 'modalidad', 
                                'err_formato_matricula']
df_inscripciones.drop(col_eliminar_inscripciones, inplace=True, axis=1)

"""**Datos de Cursado**"""

# Eliminar Columnas 
col_eliminar_academico = ['plan', 'fecha_ingreso', 'err_formato_matricula']
df_academicos.drop(col_eliminar_academico, inplace=True, axis=1)

"""**Datos Acad√©micos**"""

col_eliminar_cursado = ['err_formato_matricula',]
df_cursado.drop(col_eliminar_cursado, inplace=True, axis=1)

"""**Visualizar los Datasets Modificados**"""

print("Vista del dataset de inscripciones modificado:")
display(df_inscripciones.head(1))

print("Vista del dataset de datos acad√©micos modificado:")
display(df_academicos.head(1))

print("Vista del dataset de datos de cursado modificado:")
display(df_cursado.head(1))



"""## Limpieza

### Eliminaci√≥n de Columnas

En esta etapa, se elimar√°n las filas que no satisfacen las reglas del negocio.

**Datos de Inscripciones**
"""

# Datos de inscripciones

# Se aplica la limpieza
temp = df_inscripciones[df_inscripciones.regla_fechas_ingreso != 'err']

limpiar_columnas = ['regla_fechas_ingreso']
df_inscripciones_a = temp.drop(limpiar_columnas, axis=1)

diferencia_inscripciones_a = df_inscripciones.shape[0] - df_inscripciones_a.shape[0]
ratio_inscripciones_a = 100 - (( df_inscripciones_a.shape[0] / df_inscripciones.shape[0] )*100)
ratio_inscripciones_a = round(ratio_inscripciones_a, 2)

print(f"Filas resultantes: {df_inscripciones_a.shape[0]} ")
print(f"Total original: {df_inscripciones.shape[0]} ")
print(f"Diferencia: {diferencia_inscripciones_a} ({ratio_inscripciones_a}%)")

df_inscripciones_a.head(5)

"""**Datos de Cursado**"""

# Datos de cursado

# Se aplica la limpieza
temp = df_cursado[df_cursado.regla_estado_inscripcion != 'err']

limpiar_columnas = ['regla_estado_inscripcion', 'estado_inscripcion']

df_cursado_a = temp.drop(limpiar_columnas, axis=1)

diferencia_cursado_a = df_cursado.shape[0] - df_cursado_a.shape[0]
ratio_cursado_a = 100 - (( df_cursado_a.shape[0] / df_cursado.shape[0] )*100)
ratio_cursado_a = round(ratio_cursado_a, 2)

print(f"Filas resultantes: {df_cursado_a.shape[0]}")
print(f"Total original: {df_cursado.shape[0]}")
print(f"Diferencia: {diferencia_cursado_a} ({ratio_cursado_a}%)")

df_cursado_a.head(5)

"""**Datos Acad√©micos**"""

# Datos acad√©micos

# Se aplica la limpieza
temp = df_academicos[df_academicos.regla_verificacion_calidad != 'err']

limpiar_columnas = ['regla_verificacion_calidad']
df_academicos_a = temp.drop(limpiar_columnas, axis=1)

diferencia_academicos_a = df_academicos.shape[0] - df_academicos_a.shape[0]
ratio_academicos_a = 100 - (( df_academicos_a.shape[0] / df_academicos.shape[0] )*100)
ratio_academicos_a = round(ratio_academicos_a, 2)

print(f"Filas resultantes: {df_academicos_a.shape[0]}")
print(f"Total original: {df_academicos.shape[0]}")
print(f"Diferencia: {diferencia_academicos_a} ({ratio_academicos_a}%)")

df_academicos_a.head(5)



"""### Confirmaci√≥n de Valores Nulos

**Datos de Inscripciones**
"""

#Dataset: datos_inscripciones

nulos_x_columna_inscripciones = df_inscripciones_a.isna().sum()

print(f"Cantidad de filas que tienen valores nulos por atributo:\n{nulos_x_columna_inscripciones}")

suma_nulos_inscripciones = sum(nulos_x_columna_inscripciones)

print(f"Total de filas con valores nulos en Datos de Inscripciones: {suma_nulos_inscripciones}")

"""**Datos de Cursado**"""

#Dataset: datos_cursado

nulos_x_columna_cursado = df_cursado_a.isna().sum()

print(f"Cantidad de filas que tienen valores nulos por atributo:\n{nulos_x_columna_cursado}")

suma_nulos_cursado = sum(nulos_x_columna_cursado)

print(f"Total de filas con valores nulos en Datos de Cursado: {suma_nulos_cursado}")

"""**Datos Acad√©micos**"""

#Dataset: datos_academicos

nulos_x_columna_academicos = df_academicos_a.isna().sum()

print(f"Cantidad de filas que tienen valores nulos por atributo:\n{nulos_x_columna_academicos}")

suma_nulos_academicos = sum(nulos_x_columna_academicos)

print(f"Total de filas con valores nulos en Datos Acad√©micos: {suma_nulos_academicos}")

"""## Construcci√≥n de Atributos

**Atributo 1 a generar:** grado de cumplimiento de actividades
"""

df_academicos_a.iloc[1]

# actividades / total = % de avance en la carrera

def pct_avance_carrera(row):
    return row.actividades_aprobadas / row.total_actividades

avance_carrera = df_academicos_a.apply(lambda row: pct_avance_carrera(row), axis=1).rename('avance_carrera')
avance_carrera

"""**TODO:** evaluar la generaci√≥n de otros atributos"""

# Vista de otros atributos

#academicos_a.anio_ultima_reinscripcion.value_counts()

df_academicos_a.calidad.value_counts()

"""## Integraci√≥n de los Datos"""

#Las uniones se hacen de a pares - revisar nombres de atributos

cantidad_filas = df_inscripciones_a.shape[0]

inscripciones_x_cursadas = pd.merge(df_inscripciones_a, df_cursado_a, on='id_estudiante', how='inner')
coincidencias_uno = inscripciones_x_cursadas.shape[0]

print(f"Datos de inscripciones: {cantidad_filas} - Coincidencias con datos de cursado: {coincidencias_uno}")

#Para verificar visualmente se puede ejecutar

inscripciones_x_cursadas.head(5)

#Las uniones se hacen de a pares - revisar nombres de atributos

datos_completos = pd.merge(inscripciones_x_cursadas, df_academicos_a, on='id_estudiante', how='inner')

# Se visualiza el dataset unificado

datos_completos.head()

print(f"Datos de inscripciones: {cantidad_filas}")
print(f"Datos de cursado: {df_cursado_a.shape[0]}")
print(f"Coincidencias entre inscripciones y cursado: {coincidencias_uno}")

print(f"Datos completos (filas): {datos_completos.shape[0]} integrando todas las fuentes")
print(f"Datos completos (columnas): {datos_completos.shape[1]} integrando todas las fuentes")

"""### Exploraci√≥n del Nuevo Dataset"""

def reporte_descripcion_dataset(df):
    columnas = df.columns
    print("Columnas del dataset:\n")
    for col in columnas:
        print(col)
    print(f"\nCantidad de Filas: {df.shape[0]}")
    print(f"\nCantidad de Columnas: {datos_completos.shape[1]} ")

print("Descripci√≥n del dataset 'datos_completos'")
reporte_descripcion_dataset(datos_completos)

"""### Meta-Datos (Dtale)

Se har√° uso de la biblioteca [Dtale](https://pypi.org/project/dtale/) para obtener los Meta-Datos del dataset.
"""

# Colab
# Dtale para Datos Completos
dtale.show(datos_completos)

#comp = dtale.show(datos_completos)
#comp.open_browser()

"""### Exportar Datos Completos"""

# Exportar/Descargar Fichero
descargar_dataframe_csv(datos_completos, 'datos_completos.csv')

# Filas y Columnas del Dataset
print(f"Cantidad de Filas: {datos_completos.shape[0]}")
print(f"Cantidad de Columnas: {datos_completos.shape[1]}")

"""---
<a id=" "></a>
# üßÆ **Sprint 2: Modelado**
---

El [Modelado](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-modeling) generalmente se lleva a cabo en m√∫ltiples iteraciones. T√≠picamente, los Cient√≠ficos de Datos ejecutan varios modelos utilizando los par√°metros predeterminados y luego ajustan los par√°metros o regresan a la fase de preparaci√≥n de datos para las manipulaciones requeridas por su modelo de elecci√≥n.

En el presente proyecto, se han considerado las siguientes etapas para la fase de Modelado:

1. Selecci√≥n de la T√©cnica de Modelado

2. Adaptaci√≥n de los Datos

3. Generaci√≥n del Plan de Pruebas

4. Construcci√≥n del Modelo

5. Evaluaci√≥n del Modelo

### Importar Datos Completo
"""

# Datos Completos
# Cargar Fichero
datos_completos = cargar_csv("datos_completos.csv")

# Imprimir Primeras Filas
if datos_completos is not None:
    print("\nDatos Completos:")
    print(datos_completos.shape)
    
datos_completos.head()

# Columnas
datos_completos.columns





"""### Meta-Datos (SweetViz) de Datos Completos

Se har√° uso de la biblioteca [SweetViz](https://pypi.org/project/sweetviz/) para obtener los Meta-Datos del dataset con los Datos Completos.
"""

def reporte_descripcion_dataset(df):
    columnas = df.columns
    print("Columnas del dataset:\n")
    for col in columnas:
        print(col)
    print(f"\nCantidad de filas: {df.shape[0]}")

print("Descripci√≥n del dataset 'datos_completos'")
reporte_descripcion_dataset(datos_completos)

import warnings
import sweetviz as sw

warnings.filterwarnings('ignore')

reporte_completo = sw.analyze(datos_completos)
reporte_completo.show_html(filepath='reporte_completo.html', 
            open_browser=True, 
            layout='vertical', 
            scale=None)

"""## Selecci√≥n de la T√©cnica de Modelado

### Adaptaci√≥n de Datos Completos
"""

limpiar_columnas = ['actividades_aprobadas', 'total_actividades', 'ingreso_aprobadas', 'ingreso_libres', 
                    'ingreso_totales', 'cursadas_aprobadas', 'cursadas_regulares', 'cursadas_libres', 
                    'cursadas_totales', 'fecha_ingreso', 'fecha_inscripcion', 'anio_ingreso',
                    'inscripciones_examenes', 'examenes_aprobados',
                    'promedio_sin_aplazos', 'promedio_con_aplazos', 'id_estudiante',
                   ]

"""A continuaci√≥n, se crear√°n algunos campos calculados.

**pct_avance_ingreso**

$$ pct\_avance\_ingreso = \frac{ingreso\_aprobadas}{ingreso\_totales}
 $$
"""

# Generaci√≥n de un atributo para el rendimiento en el ingreso
def pct_avance_ingreso(row):
    return row.ingreso_aprobadas / row.ingreso_totales

avance_ingreso = datos_completos.apply(lambda row: pct_avance_ingreso(row), axis=1).rename('avance_ingreso')
avance_ingreso

"""**pct_avance_semestre**

$$ pct\_avance\_semestre = \frac{cursadas\_aprobadas + cursadas\_regulares}{cursadas\_totales}
 $$
"""

# Generaci√≥n de un atributo nuevo para el rendimiento en el primer semestre
def pct_avance_semestre(row):
    return (row.cursadas_aprobadas + row.cursadas_regulares) / row.cursadas_totales

avance_1er_semestre = datos_completos.apply(lambda row: pct_avance_semestre(row), axis=1).rename('avance_1er_semestre')
avance_1er_semestre

"""**pct_avance_carrera**

$$ pct\_avance\_carrera = \frac{actividades\_aprobadas}{total\_actividades}
 $$
"""

# Atributo generado previamente pero que no se agreg√≥
def pct_avance_carrera(row):
    return row.actividades_aprobadas / row.total_actividades

avance_carrera = datos_completos.apply(lambda row: pct_avance_carrera(row), axis=1).rename('avance_carrera')
avance_carrera

"""**examenes_1er_semestre**"""

# Atributo generado previamente pero que no se agreg√≥
def examenes_1er_semestre(row):
    if row.inscripciones_examenes > 0 and row.examenes_aprobados > 0:
        return 'A'
    elif row.inscripciones_examenes > 0:
        return 'I'
    else:
        return 'N'

examenes_1er_semestre = datos_completos.apply(lambda row: examenes_1er_semestre(row), axis=1).rename('examenes_1er_semestre')
examenes_1er_semestre

"""**rango_promedios**"""

# Se pasan los valores de los promedios a rangos
valores = ['Bajo', 'Medio', 'Alto']

datos_completos['rango_promedios']=pd.cut(x=datos_completos['promedio_con_aplazos'], 
                         bins=[0.0, 5.0, 7.0, 10.0], 
                         include_lowest=True, 
                         labels=valores)

# Se eliminan y agregan las columnas

temp = datos_completos.drop(limpiar_columnas, axis=1)
datos = pd.concat([temp, examenes_1er_semestre, avance_ingreso, avance_1er_semestre, avance_carrera], axis=1)

datos.head(5)

# Atributos/Columnas
datos.columns

"""**Observaciones**

En este punto se podr√≠an aplicar transformaciones que fueran de utilidad para que la exportaci√≥n de datos sea consistente con los requerimientos de las t√©cnicas a aplicar en la fase siguiente. Ejemplos:

* Binarizaci√≥n de atributos

### Exportar Datos Completos Filtrados
"""

#datos.to_csv('../datasets/datos_completos_filtrados.csv', sep=';', index=False)

# Exportar Datos Completos Filtrados
descargar_dataframe_csv(datos, 'datos_completos_filtrados.csv')

"""### Meta-Datos (SweetViz) Datos Completos Filtrados"""

#import warnings
#import sweetviz as sw

warnings.filterwarnings('ignore')

reporte_completo_filtrado = sw.analyze(datos)
reporte_completo_filtrado.show_html(filepath='reporte_datos_completos_filtrados.html', 
            open_browser=True, 
            layout='vertical', 
            scale=None)



"""## Construcci√≥n del Modelo

### Importar Datos Completos Filtrados
"""

# Datos Completos Filtrados
# Cargar Fichero
datos = cargar_csv("datos_completos_filtrados.csv")

# Imprimir Primeras Filas
if datos is not None:
    print("\nDatos Completos Filtrados:")
    print(datos.shape)
    
datos.head()

# Datos = Datos Completos Filtrados
datos.calidad.value_counts()

"""### Configuraci√≥n de Par√°metros

#### Labels y Features

Separaci√≥n del atributo de clase y las columnas a utilizar del dataset.
"""

# Datos = Datos Completos Filtrados
labels = datos.calidad.values

features = datos[['propuesta', 'estado_inscripcion', 'fecha_ultimo_examen',
       'anio_ultima_reinscripcion', 'regular', 'segundo_anio',
       'rango_promedios', 'examenes_1er_semestre', 'avance_ingreso',
       'avance_1er_semestre', 'avance_carrera']]

"""‚ö†Ô∏è **Warning!** ‚ö†Ô∏è

 En una posterior iteraci√≥n, buscando mejorar el modelo, se podr√° optar por prescindir de la caracter√≠stica **`regular`**, dada la alta correlaci√≥n que existe con ella.
"""

print(f"Vista de los datos del atributo target: \n{labels[:10]}")
print("-"*50)
print("Vista parcial de los datos sin el atributo target:")
features.head()



"""#### Ajustes 

Se efectuar√°n algunos ajustes de valores para un mejor rendimiento de las t√©cnicas de Machine Learning. En esencia, estas modificaciones tienen que ver con:

*  Binarizar Variables

**Mapeo**

Antes de pasar a valores binarizados se puede hacer un mapeo para tener valores m√°s representativos
"""

# Ejemplos con algunos atributos

display(features.estado_inscripcion.unique())

display(features.fecha_ultimo_examen.unique())

"""**Binarizar Atributos**

*  **estadoN**
"""

# Columna: estado_inscripcion
cambios_estado = {
    'Pendiente' : 'P', 
    'Aceptado' : 'A'
}

estadoN = features.loc[:, ('estado_inscripcion')].map(cambios_estado).rename('estadoN')

"""*  **rindio_examen**"""

def rindio_examen(row):
    if pd.isna(row.fecha_ultimo_examen):
        return 'N'
    else:
        return 'S'
    
rindio_examen = features.apply(lambda row : rindio_examen(row), axis=1).rename('rindio_examen')

"""*  **inscrito_ult_ciclo**"""

def inscrito_ult_ciclo(row):
    # para este caso el √∫ltimo ciclo lectivo es 2021
    if row.anio_ultima_reinscripcion == 2021:
        return 'S'
    else:
        return 'N'

inscrito_ult_ciclo = features.apply(lambda row : inscrito_ult_ciclo(row), axis=1).rename('inscrito_ult_ciclo')

"""*  **Concatenar Atributos Nuevos**"""

atributos_nuevos = pd.concat([estadoN, rindio_examen, inscrito_ult_ciclo], axis=1)
atributos_nuevos.head()

"""‚ö†Ô∏è **Warning!** ‚ö†Ô∏è

 En una iteraci√≥n posterior, se podr√≠a excluir el atributo **`inscrito_ult_ciclo`** buscando una mejora en el modelo.

#### Datos Procesados
"""

ajustes = ['estado_inscripcion', 'fecha_ultimo_examen', 'anio_ultima_reinscripcion']

features = features.drop(ajustes, axis=1)

datos_procesados = pd.concat([features, atributos_nuevos], axis=1)
datos_procesados.head()

"""#### Exportar Datos Procesados"""

# Exportar 
descargar_dataframe_csv(datos_procesados, 'data_procesados.csv')

"""#### Generar Data Dummies"""

data = pd.get_dummies(datos_procesados)
data.head()



# Se verifica la presencia de valores nulos
#np.isnan(data).any()

data.info()

"""#### Exportar Data Dummies

No est√° dem√°s contar con un backup de este dataset con las variables dummy. Eso podr√≠a servir como un pivote, y facilitar la iteraci√≥n de pruebas para los modelos, sin tener que reprocesar todo nuevamente.
"""

# Exportar Data (con Dummies)
descargar_dataframe_csv(data, 'data_dummies.csv')

"""### Split Train-Test

En esta secci√≥n se proceder√° a generar los datasets correspondientes al entrenamiento y testing, con ayuda de la biblioteca **`train_test_split`** de **Scikit-Learn**. Se ha optado por usar una partici√≥n correspondiente a:

*  75% de Entrenamiento

*  25% de Testing

Lo cual es totalmente parametrizable con el objeto **`test_size`**.
"""

from sklearn.model_selection import train_test_split

# Se reparten los datos disponibles en conjuntos para entrenamiento y testeo
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.25)

# Toma por defecto 75% (entrenamiento) - 25% (testeo)
# Se podr√≠a modificar con el par√°metro test_size = 0.## y el complemento para training

# Vista de los datos de entrenamiento para una fila

print(f"Un registro de entrenamiento: \n{train_data.iloc[1]}")
print("-"*50)
print(f"Label del registro: \n{train_labels[1]}")

# Vista de los datos de testeo para una fila

print(f"Un registro de testeo: \n{test_data.iloc[1]}")
print("-"*50)
print(f"Label del registro: \n{test_labels[1]}")



"""## Evaluaci√≥n del Modelo

### Funci√≥n de Evaluaci√≥n

Se usar√° la biblioteca **`sklearn.metrics`** para calcular varias m√©tricas para los modelos que ser√°n utilizados.
"""

#Importaciones varias para la evaluaci√≥n

from sklearn.metrics import confusion_matrix,accuracy_score,classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Configuraci√≥n de la evaluaci√≥n

def evaluar_modelo(modelo, test_data, test_labels, cmap='viridis'):
    tipo = type(modelo)
    hiperparametros = modelo.get_params()
    prediction = modelo.predict(test_data)
    print(f'Modelo: \n{tipo}')
    print("\n")
    print(f'Hiperpar√°metros: \n{hiperparametros}')
    print("\n")
    print('Rendimiento obtenido:',accuracy_score(test_labels,prediction))
    print("\n")
    print('Reporte de indicadores:\n',classification_report(test_labels,prediction))
    print('Matriz de confusi√≥n:')
    cm = confusion_matrix(test_labels, prediction, labels=clf.classes_)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm,
                                   display_labels=clf.classes_)
    disp.plot(cmap=cmap)
    plt.show()

"""### Aplicaci√≥n de t√©cnicas para extracci√≥n de patrones

Se ejecutar√° un set de pruebas con los siguientes modelos:

*  ZeroR

*  Regresi√≥n Log√≠stica

*  KNN

*  √Årboles (TDIDT)

*  M√©todos de Ensamblado

   *  Random Forest

   *  Gradient Boosting

*  Red Neuronal

*  Support Vector Machine (SVM)

Posteriormente, en una siguiente iteraci√≥n, se podr√°n seleccionar algunos de estos modelos, con base en los resultados obtenidos, y proseguir con variaciones en los hiperpar√°metros para intentar mejorarlos.

### Prueba 1

#### ZeroR
"""

from sklearn.dummy import DummyClassifier

clf = DummyClassifier()
clf.fit(train_data, train_labels)

prediction = clf.predict(test_data)
print('Rendimiento obtenido:',accuracy_score(test_labels, prediction))
print('Vista de una muestra de valores de predicci√≥n y datos de testeo:')
print(prediction[:10])
print(test_labels[:10])

# Para obtener todos los par√°metros y no solo los modificados

from sklearn import set_config

set_config(print_changed_only=False)

"""#### Regresi√≥n Log√≠stica"""

from sklearn.linear_model import LogisticRegression

# Prueba Nro.1
lreg = LogisticRegression(solver = 'liblinear')

lreg.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

#Prueba Nro. 1
evaluar_modelo(lreg, test_data, test_labels, cmap='viridis')

"""#### KNN"""

from sklearn.neighbors import KNeighborsClassifier

# Prueba nro. 1
knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'ball_tree', leaf_size = 25)

knn.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

# Prueba nro. 1
evaluar_modelo(knn, test_data, test_labels, cmap='viridis')

"""#### √Årboles (TDIDT)

TDIDT = Top Down Induction of Decision Trees.
"""

from sklearn.tree import DecisionTreeClassifier

# Prueba Nro. 1
dtc = DecisionTreeClassifier(max_depth=3, criterion='entropy', min_samples_split=10)

dtc.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

# Prueba Nro. 1
evaluar_modelo(dtc, test_data, test_labels, cmap='viridis')

"""**Graphviz**"""

!pip install graphviz

#from IPython.display import SVG
#from sklearn.tree import export_graphviz  
#from graphviz import Source 
# Se requiere tener la librer√≠a graphviz instalada a nivel del kernel que usa la libreta  
# Puede hacerse con: pip install graphviz | conda install -c anaconda python-graphviz pydot 

#graph = Source(export_graphviz(dtc, out_file=None, feature_names=data.columns))
#graph.format = 'png'
#graph.render('dtree_render',view=True)

"""Generar una representaci√≥n gr√°fica del √Årbol de Decisi√≥n."""

#!pip install graphviz
from IPython.display import SVG
from sklearn.tree import export_graphviz  
from graphviz import Source 

def render_tree(model, feature_names, filename='dtree_render', view=True):
    """
    Esta funci√≥n crea una representaci√≥n gr√°fica de un √°rbol de decisi√≥n utilizando graphviz.
    
    Par√°metros:
    - model: El modelo del √°rbol de decisi√≥n.
    - feature_names: Nombres de las caracter√≠sticas utilizadas en el modelo.
    - filename: Nombre del archivo en el que se guardar√° el gr√°fico (default: 'dtree_render').
    - view: Si es True, se mostrar√° el gr√°fico (default: True).
    """
    graph = Source(export_graphviz(model, out_file=None, feature_names=feature_names))
    graph.format = 'png'
    graph.render(filename, view=view)

# 
render_tree(dtc, data.columns)

"""#### M√©todos de Ensamblado: Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Prueba nro. 1
rndf = RandomForestClassifier(n_estimators=10)
rndf.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

# Prueba nro. 1
evaluar_modelo(rndf, test_data, test_labels, cmap='viridis')

"""#### M√©todos de Ensamblado: Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

# Prueba nro. 1
gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,
    max_depth=1, random_state=0)

gbc.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

# Prueba nro. 1
evaluar_modelo(gbc, test_data, test_labels, cmap='viridis')

"""#### Red Neuronal"""

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=0)
mlp.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

# Prueba nro. 1
evaluar_modelo(mlp, test_data, test_labels, cmap='viridis')

"""#### Support Vector Machines (SVM)"""

from sklearn.svm import SVC

svc = SVC(kernel='sigmoid', random_state=0)
svc.fit(train_data, train_labels)

"""**Evaluaci√≥n**"""

# Prueba nro. 1
evaluar_modelo(svc, test_data, test_labels, cmap='viridis')

"""Con tal de mejorar el rendimiento para el modelo **SVM**, se procede a usar ***RandomizedSearchCV*** para obtener los mejores par√°metros."""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
import numpy as np

# Definir la grilla de par√°metros
param_dist = {'C': np.logspace(-3, 2, 6), 
              'gamma': np.logspace(-3, 2, 6),
              'kernel': ['linear', 'rbf']}

# Instanciar el modelo
svc = SVC(random_state=0)

# Instanciar RandomizedSearchCV
random_search = RandomizedSearchCV(svc, param_distributions=param_dist, n_iter=20, cv=5, random_state=0)

# Ajustar los datos de entrenamiento
random_search.fit(train_data, train_labels)

# Ver los mejores par√°metros encontrados
print("Mejores Par√°metros para SVM: ", random_search.best_params_)

# Ver el rendimiento del mejor modelo
print("Mejor Score para SVM: ", random_search.best_score_)

"""Se utilizan los par√°metros obtenidos en la l√≠nea anterior:

*  Mejores Par√°metros para SVM:  `{'kernel': 'linear', 'gamma': 0.01, 'C': 0.1}`

"""

from sklearn.svm import SVC

# Instanciar el modelo con los mejores par√°metros
svc = SVC(kernel='linear', gamma=0.01, C=0.1, random_state=0)

# Ajustar el modelo a los datos de entrenamiento
svc.fit(train_data, train_labels)

# Prueba 1.1 SVC
evaluar_modelo(svc, test_data, test_labels, cmap='viridis')

"""‚ö†Ô∏è **Warning!**‚ö†Ô∏è

Un ***rendimiento*** de 1.0 en un modelo de machine learning a menudo indica que el modelo puede estar ***sobreajustado***, es decir, que ha aprendido los datos de entrenamiento "demasiado bien". 

Un modelo sobreajustado puede tener un rendimiento muy bueno en los datos de entrenamiento, pero tiende a generalizar mal a datos nuevos o no vistos, lo cual es un problema.

### **Prueba 2**

Se ejecutar√°n los siguientes modelos:
*  **M√©todos de Ensamblado**
   *  ***Gradient Boosting 2*** (es decir, una variante del usado en la Prueba 1)

*  **Red Neuronal 2** (es decir, una variante del usado en la Prueba 1)

#### Importar Datos Completos Filtrados
"""

# Datos Completos Filtrados
# Cargar Fichero
datos = cargar_csv("datos_completos_filtrados.csv")

# Imprimir Primeras Filas
if datos is not None:
    print("\nDatos Completos Filtrados:")
    print(datos.shape)
    
datos.head()

"""#### Importar Data Dummies

Para evitar re-ejecutar varias etapas del presente Notebook, se proceder√° a tomar como pivote el Dataset de ***Datos con Dummies***, previamente guardado para tal fin.
"""

# Datos con Dummies
# Cargar Fichero
data_dummies = cargar_csv("data_dummies.csv")

# Imprimir Primeras Filas
if data_dummies is not None:
    print("\nDatos con Dummies:")
    print(data_dummies.shape)
    
data_dummies.head()

data_dummies.columns

"""#### Dataset para Prueba 2

A partir de los Datos con Dummies, se eliminar√°n las siguientes columnas para la iteraci√≥n de la Prueba 2:

*  **`regular`**

   *  `regular_N` 
   
   *  `regular_S`

*  **`inscrito_ult_ciclo`**

   *  `inscrito_ult_ciclo_N`
   
   *  `inscrito_ult_ciclo_S`
"""

import pandas as pd

data_prueba2 = data_dummies.copy()

columnas_eliminar = ['regular_N', 'regular_S', 'inscrito_ult_ciclo_N', 'inscrito_ult_ciclo_S']

data_prueba2 = data_prueba2.drop(columnas_eliminar, axis=1)
data_prueba2.head(5)

data_prueba2.columns

"""#### Split Train-Test

Se agregar√° el par√°metro **`random_state`** a la funci√≥n **`tran_test_split`** para asegurar la reproduciblidad de los resultados. Es decir, desempe√±ar√° la funci√≥n de una semilla.
"""

# Datos = Datos Completos Filtrados
labels = datos.calidad.values

# Se quita variable 'regular'
features = datos[['propuesta', 'estado_inscripcion', 'fecha_ultimo_examen',
       'anio_ultima_reinscripcion', 'segundo_anio',
       'rango_promedios', 'examenes_1er_semestre', 'avance_ingreso',
       'avance_1er_semestre', 'avance_carrera']]

from sklearn.model_selection import train_test_split

# Se reparten los datos disponibles en conjuntos para entrenamiento y testeo
train_data2, test_data2, train_labels2, test_labels2 = train_test_split(data_prueba2, labels, test_size=0.25, random_state=42)

# Para obtener todos los Par√°metros

from sklearn import set_config

set_config(print_changed_only=False)

from sklearn.dummy import DummyClassifier

clf = DummyClassifier()
clf.fit(train_data, train_labels)

"""#### Red Neuronal 2"""

from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler

# Escalar los datos
scaler = StandardScaler()
scaler.fit(train_data2)

train_data2 = scaler.transform(train_data2)
test_data2 = scaler.transform(test_data2)

# Crear la Red Neuronal con sus Hiperpar√°metros
mlp2 = MLPClassifier(hidden_layer_sizes=(100, 50),  # Se a√±ade segunda capa oculta con 50 neuronas
                     activation='tanh',  # Se cambia la funci√≥n de activaci√≥n a 'tanh'
                     solver='sgd',  # Solucionador a 'sgd'
                     alpha=0.0001,  # Tasa de regularizaci√≥n
                     learning_rate='adaptive',  # Tasa de aprendizaje adaptativa
                     learning_rate_init=0.001,  # Establecemos la tasa de aprendizaje inicial
                     max_iter=500,  # Se aumenta el n√∫mero m√°ximo de iteraciones
                     early_stopping=True,  # Se usa parada temprana para prevenir el sobreajuste
                     random_state=0)  # Se fija una semilla para la reproducibilidad

# Entrenar la Red Neuronal
mlp2.fit(train_data2, train_labels2)

# Prueba 2
evaluar_modelo(mlp2, test_data2, test_labels2, cmap='cividis')

"""Un ***rendimiento*** de **0.7** puede parecer peor a simple vista, pero si ese rendimiento se mantiene constante o similar tanto en los datos de entrenamiento como en los de prueba (o validaci√≥n), entonces ese modelo puede ser realmente mejor en t√©rminos de su capacidad para generalizar a datos no vistos.

Es decir, la clave aqu√≠ es no s√≥lo fijarse en el rendimiento del modelo en los datos de entrenamiento, sino tambi√©n evaluar c√≥mo se comporta con los datos de prueba o validaci√≥n. 
"""



"""#### Gradient Boosting 2"""

from sklearn.ensemble import GradientBoostingClassifier

# Prueba 2
# Creamos el modelo de Gradient Boosting
gbc2 = GradientBoostingClassifier(n_estimators=200,  # Se aumenta n√∫mero de estimadores
                                  learning_rate=0.1,  # Se disminuye la tasa de aprendizaje
                                  max_depth=3,  # Se aumenta la profundidad m√°xima
                                  min_samples_split=4,  # Se establece un m√≠nimo de muestras para dividir un nodo
                                  min_samples_leaf=2,  # Se establece un m√≠nimo de muestras para ser una hoja nodo
                                  max_features='sqrt',  # Se considera la ra√≠z cuadrada del n√∫mero de caracter√≠sticas al buscar la mejor divisi√≥n
                                  random_state=0)  # Se fija una semilla para la reproducibilidad

# Entrenamiento
gbc2.fit(train_data2, train_labels2)

# Prueba 2
evaluar_modelo(gbc2, test_data2, test_labels2, cmap='cividis')

"""---
<a id=" "></a>
# üîç **Sprint 2: Evaluaci√≥n**
---

Despu√©s del Modelado, es necesario [evaluar](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-evaluation) los resultados utilizando los criterios de negocio establecidos al inicio del proyecto. Esta es la clave para asegurar que la instituci√≥n pueda hacer uso de los resultados que se han obtenido.

Con base en los resultados obtenidos de las Pruebas 1 y 2, se optar√° por aplicar los siguientes modelos en la Fase de Despligue/Implantaci√≥n, es decir, en los datos con nuevos alumnos:

*  Random Forest

*  Gradient Boosting 2

*  Red Neuronal 2

---
<a id=" "></a>
# üöÄ **Sprint 2: Despliegue**
---

#### Importar Datos Completos Filtrados
"""

# Datos Completos Filtrados
# Cargar Fichero
datos = cargar_csv("datos_completos_filtrados.csv")

# Imprimir Primeras Filas
if datos is not None:
    print("\nDatos Completos Filtrados:")
    print(datos.shape)
    
datos.head()

datos.columns

# Determinar Cantidad Valores en Variable "Calidad"
datos.calidad.value_counts()

datos.info()

"""#### Importar Data Dummies"""

# Recuperar los Datos con Dummies
# Cargar Fichero
data_dummies = cargar_csv("data_dummies.csv")

# Imprimir Primeras Filas
if data_dummies is not None:
    print("\nDatos (backup) con Dummies:")
    print(data_dummies.shape)
    
data_dummies.head()

data_dummies.info()

"""## Importar Datos Nuevos"""

# Cargar Fichero
datos_nuevos = cargar_csv("datos_nuevos_22.csv")

# Imprimir Primeras Filas
if datos_nuevos is not None:
    print("\nDatos Nuevos 2022:")
    print(datos_nuevos.shape)
    
datos_nuevos.head()

datos_nuevos.columns

datos_nuevos.info()

"""‚ö†Ô∏è **Warning!** ‚ö†Ô∏è

Recordar que si se implementan operaciones de transformaci√≥n sobre los datos ser√° necesario (posiblemente) replicarlas sobre los datos nuevos para que ambos datasets sean compatibles.

## Ajustes / Adaptaciones

### Nuevos Procesados

**Aplicar Ajustes (Binarizaci√≥n) a los Datos Nuevos**
"""

# Columna: estado_inscripcion
cambios_estado = {
    'Pendiente' : 'P', 
    'Aceptado' : 'A'
}

estadoN = datos_nuevos.loc[:, ('estado_inscripcion')].map(cambios_estado).rename('estadoN')

def rindio_examen(row):
    if pd.isna(row.fecha_ultimo_examen):
        return 'N'
    else:
        return 'S'
    
rindio_examen = datos_nuevos.apply(lambda row : rindio_examen(row), axis=1).rename('rindio_examen')

def inscrito_ult_ciclo(row):
    if row.anio_ultima_reinscripcion == 2021:
        return 'S'
    else:
        return 'N'

inscrito_ult_ciclo = datos_nuevos.apply(lambda row : inscrito_ult_ciclo(row), axis=1).rename('inscrito_ult_ciclo')

"""**Generar Datos Nuevos Procesados**"""

atributos_nuevos = pd.concat([estadoN, rindio_examen, inscrito_ult_ciclo], axis=1)

ajustes = ['estado_inscripcion', 'fecha_ultimo_examen', 'anio_ultima_reinscripcion']

datos_nuevos = datos_nuevos.drop(ajustes, axis=1)

nuevos_procesados = pd.concat([datos_nuevos, atributos_nuevos], axis=1)
nuevos_procesados.head()

nuevos_procesados['inscrito_ult_ciclo'].unique()

nuevos_procesados.columns

nuevos_procesados.info()

"""### Nuevos Codificados

**Generar Dummies con los Datos Nuevos Procesados**
"""

# Se obtienen las transformaciones de valores correspondientes
nuevos_codif = pd.get_dummies(nuevos_procesados)
nuevos_codif.head(3)

nuevos_codif.columns

nuevos_codif.info()

"""### Matching Nuevos Codificados

Se realiza un ajuste de columnas para coincidir con el dataset de entrenamiento.
"""

# Por si se viene arrastrando "data" de ejecuciones anteriores
# Deber√≠an ser el mismo DF
print('Columnas Data: ' + str(len(data.columns)))
print('Columnas Data Dummies: ' + str(len(data_dummies.columns)))

"""Asegurarse de que:
*  **`data = data_dummies`**
"""

data = data_dummies.copy()

# Comparar Columnas Nuevos Codificados vs Data
print('Columnas Datos Nuevos (Codificados): ' + str(len(nuevos_codif.columns)))
print('Columnas Datos Entrenamiento (Data Dummies): ' + str(len(data.columns)))

"""**Matching los Datos Nuevos Modificados**"""

# Dado que puede haber diferencias con el set de entrenamiento se emparejan
nuevos_codif_matched = nuevos_codif.copy()

columnas_faltantes = set(data.columns) - set(nuevos_codif_matched.columns)
display(columnas_faltantes)

for columna in columnas_faltantes:
    nuevos_codif_matched[columna] = 0 # Se agregan las columnas faltantes con valor 0

print('Datos Nuevos (Codificados): ' + str(len(nuevos_codif_matched.columns)))
print('Datos Entrenamiento (Data Dummies): ' + str(len(data.columns)))
print(set(data.columns) - set(nuevos_codif_matched.columns))

# Finalmente, se tiene que adaptar el orden de las columnas 

nuevos_codif_matched = nuevos_codif_matched[data.columns]
print(f"Forma del Nuevo Dataset (Matched): {nuevos_codif_matched.shape}\n")
nuevos_codif_matched.columns

nuevos_codif_matched.info()

# Exportar/Descargar Fichero
descargar_dataframe_csv(nuevos_codif_matched, 'nuevos_codif_matched.csv')

"""### Dataset Despliegue

Tal como se hizo en la Prueba 2, se proceder√° a la eliminaci√≥n de las siguientes columnas:

*  **`regular`**

   *  `regular_N` 
   
   *  `regular_S`

*  **`inscrito_ult_ciclo`**

   *  `inscrito_ult_ciclo_N`
   
   *  `inscrito_ult_ciclo_S`
"""

import pandas as pd

data_despliegue = data_dummies.copy()
nuevos_codif_despliegue = nuevos_codif_matched.copy()

columnas_eliminar = ['regular_N', 'regular_S', 'inscrito_ult_ciclo_N', 'inscrito_ult_ciclo_S']

data_despliegue = data_despliegue.drop(columnas_eliminar, axis=1)
nuevos_codif_despliegue = nuevos_codif_despliegue.drop(columnas_eliminar, axis=1)

print("Dataset Despliegue:\n")
data_despliegue.info()
print("\n")
print("Nuevos Codificados Despliegue:\n")
nuevos_codif_despliegue.info()



"""### Exportar Dataset Despliegue"""

# Exportar/Descargar Fichero
descargar_dataframe_csv(data_despliegue, 'data_despliegue.csv')

# Exportar/Descargar Fichero
descargar_dataframe_csv(nuevos_codif_despliegue, 'nuevos_codif_despliegue.csv')

"""## Predicci√≥n

Predicci√≥n de valores en el dataset nuevo (2021-2022).

### Importar Datos Despliegue
"""

# Cargar Fichero
data_despliegue = cargar_csv("data_despliegue.csv")

# Imprimir Primeras Filas
if data_despliegue is not None:
    print("\nDatos Despliegue:")
    print(data_despliegue.shape)
    
data_despliegue.head(3)

data_despliegue.columns

# Cargar Fichero
nuevos_codif_despliegue = cargar_csv("nuevos_codif_despliegue.csv")

# Imprimir Primeras Filas
if nuevos_codif_despliegue is not None:
    print("\nDatos Nuevos (2022) Codificados para el Despliegue:")
    print(nuevos_codif_despliegue.shape)
    
nuevos_codif_despliegue.head(3)

nuevos_codif_despliegue.columns

"""### Split Train-Test Despliegue"""

# Datos = Datos Completos Filtrados
labels = datos.calidad.values

features = datos[['propuesta', 'estado_inscripcion', 'fecha_ultimo_examen',
       'anio_ultima_reinscripcion', 'regular', 'segundo_anio',
       'rango_promedios', 'examenes_1er_semestre', 'avance_ingreso',
       'avance_1er_semestre', 'avance_carrera']]

# Data Despliegue
labels_despliegue = datos.calidad.values

# Se quita variable 'regular' y 'anio_ultima_reinscripcion'
features_despliegue = datos[['propuesta', 'estado_inscripcion', 'fecha_ultimo_examen',
       'anio_ultima_reinscripcion', 'regular', 'segundo_anio',
       'rango_promedios', 'examenes_1er_semestre', 'avance_ingreso',
       'avance_1er_semestre', 'avance_carrera']]

from sklearn.model_selection import train_test_split

# Se reparten los datos disponibles en conjuntos para entrenamiento y testeo
train_despliegue, test_despliegue, train_labels_despliegue, test_labels_despliegue = train_test_split(data_despliegue, labels_despliegue, test_size=0.25, random_state=42)

# Para obtener todos los Par√°metros

from sklearn import set_config

set_config(print_changed_only=False)

from sklearn.dummy import DummyClassifier

clf = DummyClassifier()
clf.fit(train_despliegue, train_labels_despliegue)

"""### **Predicci√≥n: Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

# Prueba nro. 1
rndf_despliegue = RandomForestClassifier(n_estimators=10)
rndf_despliegue.fit(train_despliegue, train_labels_despliegue)

# Se hace la predicci√≥n con los nuevos datos

prediccion_nuevos_rndf_despliegue = rndf_despliegue.predict(nuevos_codif_despliegue)

# Se convierten de array a dataframe para ser agregados como columna nueva en el dataset
prediccionDF_rndf_desp = pd.DataFrame(prediccion_nuevos_rndf_despliegue)

# Se agrega la nueva columna
datos_nuevos_calidad_rndf = datos_nuevos.copy()
datos_nuevos_calidad_rndf['calidad_rndf_desp'] = prediccionDF_rndf_desp

datos_nuevos_calidad_rndf.head()

# Se obtiene el conteo de valores para actualizar el documento del informe con la predicci√≥n realizada
pd.value_counts(datos_nuevos_calidad_rndf.calidad_rndf_desp)

"""**Exportar Datos Nuevos Calidad con Random Forest**"""

# Exportar/Descargar Fichero
descargar_dataframe_csv(datos_nuevos_calidad_rndf, 'datos_nuevos_calidad_rndf.csv')

"""### **Predicci√≥n: Gradient Boosting 2**"""

from sklearn.ensemble import GradientBoostingClassifier

# Creamos el modelo de Gradient Boosting
gbc_despliegue = GradientBoostingClassifier(n_estimators=200,  # Se aumenta n√∫mero de estimadores
                                  learning_rate=0.1,  # Se disminuye la tasa de aprendizaje
                                  max_depth=3,  # Se aumenta la profundidad m√°xima
                                  min_samples_split=4,  # Se establece un m√≠nimo de muestras para dividir un nodo
                                  min_samples_leaf=2,  # Se establece un m√≠nimo de muestras para ser una hoja nodo
                                  max_features='sqrt',  # Se considera la ra√≠z cuadrada del n√∫mero de caracter√≠sticas al buscar la mejor divisi√≥n
                                  random_state=0)  # Se fija una semilla para la reproducibilidad

# Entrenamiento
gbc_despliegue.fit(train_despliegue, train_labels_despliegue)

"""**Predicci√≥n con Gradient Boosting 2**"""

# Predecir
prediccion_nuevos_gbc_despliegue = gbc_despliegue.predict(nuevos_codif_despliegue)

# Se convierten de array a dataframe para ser agregados como columna nueva en el dataset
prediccionDF_gbc_desp = pd.DataFrame(prediccion_nuevos_gbc_despliegue)

# Se agrega la nueva columna
datos_nuevos_calidad_gbc = datos_nuevos.copy()
datos_nuevos_calidad_gbc['calidad_gbc_desp'] = prediccionDF_gbc_desp

datos_nuevos_calidad_gbc.head()

# Se obtiene el conteo de valores para actualizar el documento del informe con la predicci√≥n realizada
pd.value_counts(datos_nuevos_calidad_gbc.calidad_gbc_desp)

"""**Exportar Datos Nuevos Calidad con Gradient Boosting 2**"""

# Exportar/Descargar Fichero
descargar_dataframe_csv(datos_nuevos_calidad_gbc, 'datos_nuevos_calidad_gbc.csv')



"""### **Predicci√≥n: Red Neuronal 2**"""

from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler

# Escalar los datos
scaler = StandardScaler()
scaler.fit(train_despliegue)

train_desp_escalado = scaler.transform(train_despliegue)
test_desp_escalado = scaler.transform(test_despliegue)

# Crear la Red Neuronal con sus Hiperpar√°metros
mlp_despliegue = MLPClassifier(hidden_layer_sizes=(100, 50),  # Se a√±ade segunda capa oculta con 50 neuronas
                     activation='tanh',  # Se cambia la funci√≥n de activaci√≥n a 'tanh'
                     solver='sgd',  # Solucionador a 'sgd'
                     alpha=0.0001,  # Tasa de regularizaci√≥n
                     learning_rate='adaptive',  # Tasa de aprendizaje adaptativa
                     learning_rate_init=0.001,  # Establecemos la tasa de aprendizaje inicial
                     max_iter=500,  # Se aumenta el n√∫mero m√°ximo de iteraciones
                     early_stopping=True,  # Se usa parada temprana para prevenir el sobreajuste
                     random_state=0)  # Se fija una semilla para la reproducibilidad

# Entrenar la Red Neuronal
mlp_despliegue.fit(train_despliegue, train_labels_despliegue)

"""**Predicci√≥n con Red Neuronal 2**"""

# Predecir
prediccion_nuevos_mlp_despliegue = mlp_despliegue.predict(nuevos_codif_despliegue)

# Se convierten de array a dataframe para ser agregados como columna nueva en el dataset
prediccionDF_mlp_desp = pd.DataFrame(prediccion_nuevos_mlp_despliegue)

# Se agrega la nueva columna
datos_nuevos_calidad_mlp = datos_nuevos.copy()
datos_nuevos_calidad_mlp['calidad_mlp_desp'] = prediccionDF_mlp_desp

datos_nuevos_calidad_mlp.head()



# Se obtiene el conteo de valores para actualizar el documento del informe con la predicci√≥n realizada
pd.value_counts(datos_nuevos_calidad_mlp.calidad_mlp_desp)

"""**Exportar Datos Nuevos Calidad con Red Neuronal 2**"""

# Exportar/Descargar Fichero
descargar_dataframe_csv(datos_nuevos_calidad_mlp, 'datos_nuevos_calidad_mlp.csv')

"""---
<a id=" "></a>
# üìú **Conclusiones**
---

Con el presente Notebook se han completado todas etapas correspondientes a la **Metodolog√≠a CRISP-DM**, a saber:

1.	[Comprensi√≥n del Negocio](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-business-understanding)

2.	[Comprensi√≥n de los Datos](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-data-understanding)

3.	[Preparaci√≥n de los Datos](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-data-preparation)

4.	[Modelado](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-modeling)

5.	[Evaluaci√≥n](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-evaluation)

6.	[Implementaci√≥n](https://www.ibm.com/docs/en/spss-modeler/saas?topic=guide-deployment)

**√Åreas de Oportunidad**

- Se detectaron en el dataset datos_academicos los siguientes atributos presentan valores nulos:
  *  fecha_ultimo_examen = 406 filas nulas
  *  anio_ultima_reinscripcion = 206 filas nulas
  *  Total = 612 

La calidad de estos atributos deber√≠a formar parte de la estrategia de calidad de datos de la instituci√≥n.

**Recomendaciones**

- Implementar un sistema de ***control de versiones*** para el c√≥digo y los modelos desarrollados, lo que puede facilitar el seguimiento de los cambios y permitir la reproducci√≥n de los resultados.

-  Establecer una metodolog√≠a m√°s rigurosa para el ***dise√±o de experimentos***, lo que podr√≠a ayudar a mejorar la calidad y la eficacia de las pruebas realizadas.

-  Planificar ***revisiones peri√≥dicas post-despliegue*** para asegurar que el modelo contin√∫a funcionando como se espera a medida que los datos evolucionan con el tiempo.

---
<a id="7"></a>
# üìö **Bibliograf√≠a**
---

1. 	Chandrasekara, Chaminda; Herath, Pushpa (2019). ***Hands-On Azure Boards. Configuring and Customizing Process Workflows in Azure Devops Services***. Apress.

2.  Gothelf, Jeff (2017). ***Lean vs Agile vs Design Thinking***. Glenn Rock, NJ.

3. 	Green, David (2016). ***SCRUM. Novice to Ninja***. SitePoint Pty. Ltd.

4. 	Hundhausen, Richard (2021). ***Professional Scrum Development with Azure Devops***. Microsoft Press.

5. 	Rossberg, Joachim (2019). ***Agile Project Management with Azure Devops: Concepts, Templates, and Metrics***. Apress.

6. 	Scotcher, Edward (2015). ***Brilliant Agile Project Management. A Practical Guide to Using Agile, Scrum and Kanban***. Pearson.

7. 	Sutherland, Jeff (2014). ***SCRUM. The Art of Doing Twice the Work in Half the Time***. Crown Business.

8.  IBM. (s.f.). ***SPSS Modeler: Documentaci√≥n***. Recuperado de https://www.ibm.com/docs/en/spss-modeler/saas.
"""